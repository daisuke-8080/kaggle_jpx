{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Fwg28-0n8C8mkx1_vM3XMp1ByeK0jQID",
      "authorship_tag": "ABX9TyOohQEhwf5+ZMIllcARC8dQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisuke-8080/kaggle_jpx/blob/main/%E6%A0%AA%E4%BE%A1%E4%BA%88%E6%B8%AC%E7%94%A8_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kpoxambd9WJT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/kaggle/informer_train_csv/1301.csv')"
      ],
      "metadata": {
        "id": "1ESVbIKYA1Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class src_train(Dataset):\n",
        "  def __init__(self, df):\n",
        "    super().__init__()\n",
        "\n",
        "    pred_len = 30\n",
        "    self.df = df[:-(pred_len)]\n",
        "    self.len = len(self.df)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    dataset = self.df[['Open','High','Low','Close']].iloc[index].values\n",
        "\n",
        "    return dataset\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class tgt_train(Dataset):\n",
        "  def __init__(self, df):\n",
        "    super().__init__()\n",
        "    self.df = df\n",
        "\n",
        "    index_list = []\n",
        "    count = 0\n",
        "    pred_len = 30\n",
        "    for i in range(len(df)):\n",
        "      count += 1\n",
        "      seq_len = 60\n",
        "      if count % seq_len == 0 and count <= len(df) - pred_len:\n",
        "        index_list.append(i)\n",
        "    \n",
        "    df_ = pd.DataFrame()\n",
        "    for i in index_list:\n",
        "      df_x  = self.df.iloc[i : i + pred_len][['Open','High','Low','Close']]\n",
        "      df_ = pd.concat((df_, df_x), axis=0)\n",
        "    \n",
        "    self.df_ = df_\n",
        "\n",
        "    self.len = len(df_)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    tgt = self.df_.iloc[index][['Open','High','Low','Close']].values\n",
        "\n",
        "    return tgt"
      ],
      "metadata": {
        "id": "ucBu4q6DSXwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_ex_src = src_train(df)\n",
        "dataset_ex_tgt = tgt_train(df)"
      ],
      "metadata": {
        "id": "PKSU7wknDkhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE=60\n",
        "\n",
        "src_loader = DataLoader(dataset_ex_src, batch_size=BATCH_SIZE, drop_last=True, pin_memory=True)\n",
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE=30\n",
        "\n",
        "tgt_loader = DataLoader(dataset_ex_tgt, batch_size=BATCH_SIZE, drop_last=True, pin_memory=True)"
      ],
      "metadata": {
        "id": "nR--lfw5KogX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch import nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model,max_len, device):\n",
        "      super().__init__()\n",
        "      self.d_model = d_model\n",
        "      self.max_len= max_len\n",
        "      self.position_tensor = self.get_position_tensor().to(device=device)\n",
        "\n",
        "  def get_positional_encoding(self,  pos, i):\n",
        "      w = pos / ((10000**(2*i))/self.d_model)\n",
        "      if i % 2  ==  0:\n",
        "        return np.sin(w)\n",
        "      else:\n",
        "        return np.cos(w)\n",
        "\n",
        "  def get_position_tensor(self):\n",
        "      position_list = [[self.get_positional_encoding(pos, i) for i in range(1,self.d_model +1)] for pos in range(1,  self.max_len +1)]\n",
        "      return torch.tensor(np.array(position_list)).float()\n",
        "\n",
        "  def forward(self, x):\n",
        "      seq_len = x.size(1)\n",
        "      return x + self.position_tensor[:seq_len, :].unsqueeze(0).to(device=device)\n",
        "\n",
        "\n",
        "class Scaled_dot_product_attention(nn.Module):\n",
        "  def __init__(self, d_k):\n",
        "      super().__init__()\n",
        "      self.d_k = d_k\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "      scaler = np.sqrt(self.d_k)\n",
        "      attention_weight = torch.matmul(q, torch.transpose(k,1,2)) / scaler\n",
        "\n",
        "      if mask is not None:\n",
        "          if mask.dim() != attention_weight.dim():\n",
        "              raise ValueError(\n",
        "                  \"mask.dim != attention_weight.dim, mask.dim={}, attention_weight.dim={}\".format(\n",
        "                      mask.dim(), attention_weight.dim()\n",
        "                    )\n",
        "                )\n",
        "          attention_weight = attention_weight.data.masked_fill_(mask, -torch.finfo(torch.float).max)\n",
        "          attention_weight = attention_weight.to(device=device)\n",
        "      attention_weight = nn.functional.softmax(attention_weight, dim=2)\n",
        "\n",
        "      return torch.matmul(attention_weight, v)\n",
        "      \n",
        "\n",
        "class Multihead_attention(nn.Module):\n",
        "  def __init__(self, d_model, heads_num):\n",
        "      super().__init__()\n",
        "      self.h = heads_num\n",
        "      self.d_model = d_model\n",
        "      self.d_k = d_model // self.h\n",
        "      self.d_v = d_model // self.h\n",
        "\n",
        "      self.q_W = nn.Parameter(torch.Tensor(self.h, d_model, self.d_k).to(device=device))\n",
        "      self.k_W = nn.Parameter(torch.Tensor(self.h, d_model, self.d_k).to(device=device))\n",
        "      self.v_W = nn.Parameter(torch.Tensor(self.h, d_model, self.d_v).to(device=device))\n",
        "\n",
        "      self.scaled_dot_product_attention = Scaled_dot_product_attention(self.d_k)\n",
        "      self.linear = nn.Linear(self.h* self.d_v, d_model)\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "      batch_size, seq_len = q.size(0),q.size(1)\n",
        "\n",
        "      q = q.repeat(self.h, 1,1,1)\n",
        "      k = k.repeat(self.h, 1,1,1)\n",
        "      v = v.repeat(self.h, 1,1,1)\n",
        "\n",
        "      q = torch.einsum('hijk, hkl ->hijl', q, self.q_W)\n",
        "      k = torch.einsum('hijk, hkl ->hijl', k, self.k_W)\n",
        "      v = torch.einsum('hijk, hkl ->hijl', v, self.v_W)\n",
        "\n",
        "      q = q.view(self.h * batch_size, seq_len, self.d_k)\n",
        "      k = k.view(self.h * batch_size, seq_len, self.d_k)\n",
        "      v = v.view(self.h * batch_size, seq_len, self.d_v)\n",
        "\n",
        "      if mask is not None:\n",
        "        mask = mask.repeat(self.h, 1,1).to(device=device)\n",
        "\n",
        "      attention_output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "      attention_output = attention_output.to(device=device)\n",
        "      attention_output = torch.chunk(attention_output, self.h, dim=0)\n",
        "      attention_output = torch.cat(attention_output, dim=2)\n",
        "\n",
        "      output = self.linear(attention_output)\n",
        "      return output\n",
        "\n",
        "\n",
        "class ffn(nn.Module):\n",
        "  def __init__(self, d_model, d_ff):\n",
        "      super().__init__()\n",
        "      self.linear1= nn.Linear(d_model, d_ff)\n",
        "      self.linear2 = nn.Linear(d_ff, d_model)\n",
        "    \n",
        "  def forward(self, x):\n",
        "      return self.linear2(nn.functional.relu((self.linear1(x))))\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def  __init__(self, d_model, d_ff,  heads_num, dropout_rate, layer_norm_eps):\n",
        "      super().__init__()\n",
        "      self.multi_head_attention = Multihead_attention(d_model, heads_num)\n",
        "      self.dropout_self_attention = nn.Dropout(dropout_rate)\n",
        "      self.layer_norm_self_attention = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "\n",
        "      self.ffn = ffn(d_model,d_ff)\n",
        "      self.dropout_ffn = nn.Dropout(dropout_rate)\n",
        "      self.layer_norm_ffn =  nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "\n",
        "  def __self_attention_block(self, x, mask=None):\n",
        "      x = self.multi_head_attention(x,x,x, mask)\n",
        "      x = self.dropout_self_attention(x)\n",
        "      return x\n",
        "\n",
        "  def __feed_forward_block(self,x):\n",
        "      x = self.ffn(x)\n",
        "      x = self.dropout_ffn(x)\n",
        "      return x\n",
        "    \n",
        "  def forward(self,x, mask=None):\n",
        "      #x = self.layer_norm_self_attention(self.__self_attention_block(x, mask) + x)\n",
        "      x = self.__self_attention_block(x, mask) + x\n",
        "      #x = self.layer_norm_ffn(self.__feed_forward_block(x) + x)\n",
        "      x = self.__feed_forward_block(x) + x\n",
        "      return x\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, max_len, d_model, d_ff, heads_num, dropout_rate, layer_norm_eps, N, device):\n",
        "      super().__init__()\n",
        "      self.positional_encoding = PositionalEncoding(d_model, max_len, device=device) \n",
        "      self.encoder_layers = nn.ModuleList(\n",
        "          [TransformerEncoderLayer(d_model, d_ff,  heads_num, dropout_rate, layer_norm_eps) \n",
        "          for _ in range(N)]\n",
        "      )\n",
        "  def forward(self, x, mask=None):\n",
        "        x = self.positional_encoding(x)\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "          x = encoder_layer(x, mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "from torch.nn.modules.normalization import LayerNorm\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, d_ff,  heads_num, dropout_rate, layer_norm_eps):\n",
        "      super().__init__() \n",
        "\n",
        "      self.self_attention = Multihead_attention(d_model, heads_num)\n",
        "      self.dropout_self_attention = nn.Dropout(dropout_rate)\n",
        "      self.layer_norm_self_attention = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "\n",
        "      self.src_tgt_attention = Multihead_attention(d_model, heads_num)\n",
        "      self.dropout_src_tgt_attention = nn.Dropout(dropout_rate)\n",
        "      self.layer_norm_src_tgt_attention = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "\n",
        "      self.ffn = ffn(d_model, d_ff)\n",
        "      self.dropout_ffn = nn.Dropout(dropout_rate)\n",
        "      self.layer_norm_ffn = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
        "\n",
        "  def __src_tgt_attention_block(self, src, tgt, mask=None):\n",
        "      return self.dropout_src_tgt_attention(self.src_tgt_attention(tgt, src, src, mask))\n",
        "\n",
        "  def __self_attention_block(self, x, mask=None):\n",
        "      return self.dropout_self_attention(self.self_attention(x, x, x, mask))\n",
        "\n",
        "  def __feed_forward_block(self,x):\n",
        "      return self.dropout_ffn(self.ffn(x))\n",
        "\n",
        "  def forward(self, tgt, src, src_tgt_mask, self_mask=None):\n",
        "      #tgt = self.layer_norm_self_attention(tgt + self.__self_attention_block(tgt, self_mask))\n",
        "      tgt = tgt + self.__self_attention_block(tgt, self_mask)\n",
        "      #x = self.layer_norm_src_tgt_attention(tgt + self.__src_tgt_attention_block(src, tgt, src_tgt_mask))\n",
        "      x = tgt + self.__src_tgt_attention_block(src, tgt, src_tgt_mask)\n",
        "      #x = self.layer_norm_ffn(x + self.__feed_forward_block(x))\n",
        "      x = x + self.__feed_forward_block(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "  def  __init__(self, max_len, d_model, d_ff, heads_num, dropout_rate, layer_norm_eps, N, device):\n",
        "      super().__init__()\n",
        "      self.positional_encoding = PositionalEncoding(d_model, max_len, device)\n",
        "      self.decoder_layers = nn.ModuleList(\n",
        "          [TransformerDecoderLayer(d_model, d_ff, heads_num, dropout_rate, layer_norm_eps)\n",
        "          for _ in range(N)]\n",
        "      )\n",
        "\n",
        "  def forward(self, src, tgt, src_tgt_mask, self_mask=None):\n",
        "      tgt = self.positional_encoding(tgt)\n",
        "      for decoder_layer in self.decoder_layers:\n",
        "        tgt = decoder_layer(tgt, src, src_tgt_mask, self_mask=None)\n",
        "      return tgt\n",
        "\n",
        "\n",
        "class MyTransformer(nn.Module):\n",
        "  def __init__(self, tgt_size, max_len_enc, max_len_dec, d_model_enc, d_model_dec, heads_num, d_ff, dropout_rate = 0.1, layer_norm_eps=1e-5, N=6, device=device):\n",
        "      super().__init__()\n",
        "      self.max_len_enc = max_len_enc\n",
        "      self.max_len_dec = max_len_dec\n",
        "      self.d_model_enc = d_model_enc\n",
        "      self.d_model_dec = d_model_dec\n",
        "\n",
        "      self.heads_num = heads_num\n",
        "      self.d_ff = d_ff\n",
        "      self.dropout_rate = dropout_rate\n",
        "      self.layer_norm_eps = layer_norm_eps\n",
        "      self.N = N\n",
        "      self.device = device\n",
        "      self.tgt_size = tgt_size\n",
        "\n",
        "      self.encoder = TransformerEncoder(max_len_enc, d_model_enc, heads_num, d_ff, N=6, dropout_rate = 0.1, layer_norm_eps=1e-5, device=device)\n",
        "      self.decoder = TransformerDecoder(max_len_dec, d_model_dec, heads_num, d_ff, N=6, dropout_rate = 0.1, layer_norm_eps=1e-5, device=device)\n",
        "      self.linear = nn.Linear(d_model_dec, tgt_size)\n",
        "  \n",
        "  def _subsequent_mask(self, x):\n",
        "      batch_size = x.size(0)\n",
        "      max_len = x.size(1)\n",
        "      mask =  torch.tril(torch.ones(batch_size, max_len, max_len)).eq(0).to(self.device)\n",
        "      return  mask\n",
        "\n",
        "  def _pad_mask(self, x):\n",
        "      seq_len = x.size(1)\n",
        "      d_model = x.size(2)\n",
        "      x = x.squeeze(0)\n",
        "      mask = [[np.nan for i in range(d_model)] for j in range(seq_len)]\n",
        "      mask = torch.tensor(np.array(mask)).to(device=device)\n",
        "      mask = x.eq(mask)\n",
        "      mask = torch.any(mask == True, axis=1)\n",
        "      mask.size()\n",
        "      mask = torch.unsqueeze(mask,0)\n",
        "      mask = torch.unsqueeze(mask, 0)\n",
        "      mask = mask.repeat(1, seq_len, 1)\n",
        "      mask.size()\n",
        "      return mask.to(device=device)\n",
        "\n",
        "  def forward(self, src, tgt):\n",
        "      pad_mask_src = self._pad_mask(src)\n",
        "      src = self.encoder(src, pad_mask_src)\n",
        "      mask_self_attn = torch.logical_or(\n",
        "            self._subsequent_mask(tgt), self._pad_mask(tgt))\n",
        "      output = self.decoder(src, tgt, mask_self_attn, pad_mask_src)\n",
        "      y = self.linear(output)\n",
        "      return  output"
      ],
      "metadata": {
        "id": "qGTjO0C5wWY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PredError(Exception):\n",
        "  pass\n",
        "\n",
        "class LossError(Exception):\n",
        "  pass\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = MyTransformer(tgt_size=4, max_len_enc=60, max_len_dec=30, d_model_enc=4, d_model_dec=4, heads_num=2, d_ff=2048, dropout_rate=0.1, layer_norm_eps=0.0001, N=6, device=device)\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08)\n",
        "criterion = nn.MSELoss()\n",
        "epochs = 10\n",
        "\n",
        "for epoch  in range(epochs):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    count  = 0\n",
        "\n",
        "    for (src, tgt) in zip(src_loader, tgt_loader):\n",
        "      count += 1\n",
        "      \n",
        "      src = src.float().unsqueeze(0).to(device=device)\n",
        "      tgt = tgt.float().unsqueeze(0).to(device=device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if torch.isnan(src).any() or torch.isnan(tgt).any():\n",
        "        continue\n",
        "      \n",
        "      else:\n",
        "\n",
        "        if count >1:\n",
        "          previous_pred = pred\n",
        "\n",
        "        pred = model(src, tgt)\n",
        "\n",
        "        if torch.isnan(pred).any():\n",
        "          print(count)\n",
        "          print(previous_pred)\n",
        "          print(pred)\n",
        "          print(pred.size())\n",
        "          raise PredError('pred == nan')\n",
        "\n",
        "        tgt_ = tgt[:, 1:, :]\n",
        "        pred_ = pred[:, :-1, :]\n",
        "        \n",
        "        loss = criterion(tgt_, pred_)\n",
        "\n",
        "        if torch.isnan(loss).any():\n",
        "          print(count)\n",
        "          print(previous_pred)\n",
        "          print(pred)\n",
        "          print(pred.size())\n",
        "          raise LossError('loss == nan')\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"[train] epoch: {epoch+1}/{epochs}, epoch_loss: {loss.item():.3f}\")\n",
        "\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc193ed0-ad78-4a6d-9abe-8662498a09b7",
        "id": "Y-9zrtDbGnOf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] epoch: 1/10, epoch_loss: 143498.547\n",
            "[train] epoch: 2/10, epoch_loss: 16577.256\n",
            "[train] epoch: 3/10, epoch_loss: 1213.323\n",
            "[train] epoch: 4/10, epoch_loss: 860.718\n",
            "[train] epoch: 5/10, epoch_loss: 841.663\n",
            "[train] epoch: 6/10, epoch_loss: 849.093\n",
            "[train] epoch: 7/10, epoch_loss: 843.712\n",
            "[train] epoch: 8/10, epoch_loss: 841.792\n",
            "[train] epoch: 9/10, epoch_loss: 846.968\n",
            "[train] epoch: 10/10, epoch_loss: 840.146\n",
            "tensor([[[3034.2700, 3043.5879, 3012.5332, 3042.2500],\n",
            "         [3034.2781, 3058.5615, 3022.7634, 3046.7725],\n",
            "         [3049.2825, 3063.3105, 3032.6416, 3061.8052],\n",
            "         [3068.7014, 3093.8037, 3057.2126, 3087.2495],\n",
            "         [3094.2871, 3092.9004, 3067.8398, 3086.8052],\n",
            "         [3088.8179, 3088.3054, 3042.7634, 3068.1055],\n",
            "         [3039.4036, 3099.0679, 3033.5059, 3097.4675],\n",
            "         [3094.3760, 3128.5798, 3083.4949, 3126.5911],\n",
            "         [3129.0056, 3138.8037, 3103.1868, 3136.1060],\n",
            "         [3114.2781, 3118.8330, 3072.4265, 3098.0229],\n",
            "         [3099.0022, 3124.0679, 3082.7634, 3122.9897],\n",
            "         [3129.1189, 3134.0679, 3103.9285, 3132.4646],\n",
            "         [3104.2781, 3103.5371, 3048.1660, 3052.0266],\n",
            "         [3053.9651, 3063.3970, 3003.4597, 3007.4658],\n",
            "         [3014.4036, 3068.8303, 3007.7588, 3067.3950],\n",
            "         [3069.4036, 3088.4282, 3052.6416, 3067.1951],\n",
            "         [3059.2805, 3098.5366, 3037.5332, 3072.2629],\n",
            "         [3018.8179, 3059.0679, 3002.9719, 3056.7520],\n",
            "         [3064.4036, 3088.3054, 3018.1602, 3022.2576],\n",
            "         [3013.5374, 3024.0679, 2983.9597, 2997.6482],\n",
            "         [3014.0242, 3033.3147, 2988.7422, 3000.6272],\n",
            "         [2985.6946, 3008.8196, 2949.7327, 2953.4626],\n",
            "         [2987.3406, 3000.5637, 2965.2239, 2979.6809],\n",
            "         [2984.1438, 3028.8147, 2977.5010, 3012.6809],\n",
            "         [3039.2700, 3038.8293, 2972.3730, 3000.6809],\n",
            "         [3029.2302, 3053.1411, 3002.2786, 3047.4788],\n",
            "         [3053.4102, 3053.5686, 3007.7554, 3012.4675],\n",
            "         [3034.2727, 3033.3289, 2994.0027, 2999.4800],\n",
            "         [3003.8179, 3013.5647, 2982.7422, 2995.6272],\n",
            "         [3024.2642, 3054.0679, 3002.1904, 3052.1975]]], device='cuda:0',\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_pred = df[['Open','High','Low','Close']].iloc[-90:-30].values\n",
        "tgt_pred = df[['Open','High','Low','Close']].iloc[-30:].values\n",
        "src_pred = torch.tensor(src_pred).unsqueeze(0).float().to(device)\n",
        "tgt_pred = torch.tensor(tgt_pred).unsqueeze(0).float().to(device)\n",
        "src_of_src = torch.cat((src_pred, tgt_pred), dim=1)\n",
        "\n",
        "pred_len =20\n",
        "count = 0\n",
        "model.eval()\n",
        "for i in range(pred_len):\n",
        "  count += 1\n",
        "  pred_x = model(src_pred, tgt_pred)\n",
        "  next_1 =  pred_x[:,-1:,:]\n",
        "  src_of_src = torch.cat((src_of_src, next_1), 1)\n",
        "  src_pred = src_of_src[:,count:count+60,:]\n",
        "  tgt_pred = pred_x\n",
        "  prediction_a = src_of_src[:,-count:,:]\n",
        "  \n",
        "prediction_a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYwtw3_ZF6jA",
        "outputId": "6e6b1ef1-d7fa-409b-96ee-ceaebd1f5ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[2987.0603, 2986.6592, 2962.8809, 2984.4114],\n",
              "         [2991.1206, 2990.3184, 2960.7617, 2986.8228],\n",
              "         [2995.1809, 2993.9775, 2958.6426, 2989.2341],\n",
              "         [2999.2412, 2997.6367, 2956.5234, 2991.6455],\n",
              "         [3003.3015, 3001.2959, 2954.4043, 2994.0569],\n",
              "         [3007.3618, 3004.9551, 2952.2852, 2996.4683],\n",
              "         [3011.4221, 3008.6143, 2950.1660, 2998.8796],\n",
              "         [3015.4824, 3012.2734, 2948.0469, 3001.2910],\n",
              "         [3019.5427, 3015.9326, 2945.9277, 3003.7024],\n",
              "         [3023.6030, 3019.5918, 2943.8086, 3006.1138],\n",
              "         [3027.6633, 3023.2510, 2941.6895, 3008.5251],\n",
              "         [3031.7236, 3026.9102, 2939.5703, 3010.9365],\n",
              "         [3035.7839, 3030.5693, 2937.4512, 3013.3479],\n",
              "         [3039.8442, 3034.2285, 2935.3320, 3015.7593],\n",
              "         [3043.9045, 3037.8877, 2933.2129, 3018.1707],\n",
              "         [3047.9648, 3041.5469, 2931.0938, 3020.5820],\n",
              "         [3052.0251, 3045.2061, 2928.9746, 3022.9934],\n",
              "         [3056.0854, 3048.8652, 2926.8555, 3025.4048],\n",
              "         [3060.1458, 3052.5244, 2924.7363, 3027.8162],\n",
              "         [3064.2061, 3056.1836, 2922.6172, 3030.2275]]], device='cuda:0',\n",
              "       grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_pred_b = df[['Open','High','Low','Close']].iloc[-61:-1].values\n",
        "tgt_pred_b = df[['Open','High','Low','Close']].iloc[-1:].values\n",
        "src_pred_b = torch.tensor(src_pred_b).unsqueeze(0).float().to(device)\n",
        "tgt_pred_b = torch.tensor(tgt_pred_b).unsqueeze(0).float().to(device)\n",
        "src_of_src_b = torch.cat((src_pred_b, tgt_pred_b), dim=1)\n",
        "nans = np.zeros((1,29,4))\n",
        "nans[:,:,:] = np.nan\n",
        "nans = torch.tensor(nans).float().to(device)\n",
        "tgt_pred_b = torch.cat((tgt_pred_b, nans), dim=1)\n",
        "tgt_pred_b"
      ],
      "metadata": {
        "id": "30eXtolHKu6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_len = 20\n",
        "count = 0\n",
        "model.eval()\n",
        "for i in range(pred_len+1):\n",
        "  count += 1\n",
        "  prediction_b = model(src_pred_b, tgt_pred_b)\n",
        "  tgt_pred_b[:,count:count+1,:] = pred_x[:,count-1:count,:]\n",
        "\n",
        "prediction_b = prediction_b[:,1:pred_len+1,:]"
      ],
      "metadata": {
        "id": "cURdTXY2zi-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQXG3XCp2VNG",
        "outputId": "e9732d89-86fa-4e76-d7b4-781389bd308d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[3115.2664, 3116.8428, 2965.4980, 3060.6389],\n",
              "         [3095.2664, 3116.8428, 2960.4980, 3060.6389],\n",
              "         [3080.2664, 3081.8428, 2944.4980, 3040.6389],\n",
              "         [3065.2664, 3096.8428, 2935.4980, 3042.6389],\n",
              "         [3071.2664, 3091.8428, 2941.4980, 3060.6389],\n",
              "         [3095.2664, 3131.8428, 2941.4980, 3046.6389],\n",
              "         [3082.2664, 3126.8428, 2947.4980, 3100.6389],\n",
              "         [3170.2664, 3161.8428, 3010.4980, 3135.6389],\n",
              "         [3170.2664, 3176.8428, 3020.4980, 3130.6389],\n",
              "         [3160.2664, 3326.8428, 3030.4980, 3300.6389],\n",
              "         [3235.2664, 3236.8428, 3060.4980, 3160.6389],\n",
              "         [3160.2664, 3196.8428, 3020.4980, 3115.6389],\n",
              "         [3150.2664, 3161.8428, 2985.4980, 3080.6389],\n",
              "         [3120.2664, 3126.8428, 2990.4980, 3100.6389],\n",
              "         [3120.2664, 3131.8428, 2980.4980, 3095.6389],\n",
              "         [3120.2664, 3186.8428, 2990.4980, 3160.6389],\n",
              "         [3185.2664, 3206.8428, 3040.4980, 3145.6389],\n",
              "         [3185.2664, 3196.8428, 3050.4980, 3170.6389],\n",
              "         [3215.2664, 3206.8428, 3015.4980, 3110.6389],\n",
              "         [3145.2664, 3156.8428, 2990.4980, 3095.6389]]], device='cuda:0',\n",
              "       grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1KRsVmBq2tAm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}